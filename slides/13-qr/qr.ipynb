{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: QR Decomposition and Orthogonalization\n",
    "subtitle: Biostat/Biomath M257\n",
    "author: Dr. Hua Zhou @ UCLA\n",
    "date: today\n",
    "format:\n",
    "  html:\n",
    "    theme: cosmo\n",
    "    embed-resources: true\n",
    "    number-sections: true\n",
    "    toc: true\n",
    "    toc-depth: 4\n",
    "    toc-location: left\n",
    "    code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System information (for reproducibility):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.10.2\n",
      "Commit bd47eca2c8a (2024-03-01 10:14 UTC)\n",
      "Build Info:\n",
      "  Official https://julialang.org/ release\n",
      "Platform Info:\n",
      "  OS: macOS (arm64-apple-darwin22.4.0)\n",
      "  CPU: 12 Ã— Apple M2 Max\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-15.0.7 (ORCJIT, apple-m1)\n",
      "Threads: 8 default, 0 interactive, 4 GC (on 8 virtual cores)\n",
      "Environment:\n",
      "  JULIA_NUM_THREADS = 8\n",
      "  JULIA_EDITOR = code\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/Documents/github.com/ucla-biostat-257/2024spring/slides/13-qr/Project.toml`\n",
      "  \u001b[90m[37e2e46d] \u001b[39mLinearAlgebra\n",
      "  \u001b[90m[9a3f8284] \u001b[39mRandom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Documents/github.com/ucla-biostat-257/2024spring/slides/13-qr`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "\n",
    "Pkg.activate(pwd())\n",
    "Pkg.instantiate()\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "* We learnt Cholesky decomposition as **one** approach for solving linear regression.\n",
    "\n",
    "* A second approach for linear regression uses QR decomposition.  \n",
    "    **This is how the `lm()` function in R does linear regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " 0.327531097773397\n",
       " 0.26321470081611203\n",
       " 0.9829498789610566"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Random\n",
    "\n",
    "Random.seed!(257) # seed\n",
    "\n",
    "n, p = 5, 3\n",
    "X = randn(n, p) # predictor matrix\n",
    "y = randn(n)    # response vector\n",
    "\n",
    "# now X is a tall matrix; this function finds the least squares solution\n",
    "X \\ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to understand what is QR and how it is used to solve least squares problem.\n",
    "\n",
    "## Definition\n",
    "\n",
    "* Assume $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ has full column rank.  \n",
    "\n",
    "* **Full QR decomposition**:  \n",
    "$$\n",
    "    \\mathbf{X} = \\mathbf{Q} \\mathbf{R},  \n",
    "$$\n",
    "where  \n",
    "    - $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$, $\\mathbf{Q}^T \\mathbf{Q} = \\mathbf{I}_n$. In other words, $\\mathbf{Q}$ is an orthogonal matrix.  \n",
    "        - First $p$ columns of $\\mathbf{Q}$ form an orthonormal basis of $\\mathcal{C}(\\mathbf{X})$ (**column space** of $\\mathbf{X}$)      \n",
    "        - Last $n-p$ columns of $\\mathbf{Q}$ form an orthonormal basis of $\\mathcal{N}(\\mathbf{X}^T)$ (**null space** of $\\mathbf{X}^T$)\n",
    "    - $\\mathbf{R} \\in \\mathbb{R}^{n \\times p}$  is upper triangular with positive diagonal entries.  \n",
    "    \n",
    "* **Skinny (or thin) QR decomposition**:\n",
    "$$\n",
    "    \\mathbf{X} = \\mathbf{Q}_1 \\mathbf{R}_1,\n",
    "$$\n",
    "where\n",
    "    - $\\mathbf{Q}_1 \\in \\mathbb{R}^{n \\times p}$, $\\mathbf{Q}_1^T \\mathbf{Q}_1 = \\mathbf{I}_p$. In other words, $\\mathbf{Q}_1$ has orthonormal columns.    \n",
    "    - $\\mathbf{R}_1 \\in \\mathbb{R}^{p \\times p}$  is an upper triangular matrix with positive diagonal entries.  \n",
    "    \n",
    "* Given QR decompositin $\\mathbf{X} = \\mathbf{Q} \\mathbf{R}$,\n",
    "$$\n",
    "    \\mathbf{X}^T \\mathbf{X} = \\mathbf{R}^T \\mathbf{Q}^T \\mathbf{Q} \\mathbf{R} = \\mathbf{R}^T \\mathbf{R}.\n",
    "$$\n",
    "Therefore $\\mathbf{R}$ is the same as the upper triangular Choleksy factor of the **Gram matrix** $\\mathbf{X}^T \\mathbf{X}$.\n",
    "\n",
    "* There are 3 algorithms to compute QR: (modified) Gram-Schmidt, Householder transform, (fast) Givens transform.\n",
    "\n",
    "* In particular, the **Householder transform** for QR is implemented in LAPACK and thus used in R, Julia, Matlab, Python/Numpy, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "### QR by Gram-Schmidt (only produce thin QR)\n",
    "\n",
    "<img src=\"Gram.jpg\" width=\"150\" align=\"center\"/>\n",
    "\n",
    "<img src=\"Schmidt.jpg\" width=\"150\" align=\"center\"/>\n",
    "\n",
    "<img src=\"qr.png\" width=\"450\" align=\"center\"/>\n",
    "\n",
    "* Assume $\\mathbf{X} = (\\mathbf{x}_1, \\ldots, \\mathbf{x}_p) \\in \\mathbb{R}^{n \\times p}$ has full column rank. \n",
    "\n",
    "* Gram-Schmidt (GS) algorithm produces the **skinny QR**: \n",
    "$$\n",
    "    \\mathbf{X} = \\mathbf{Q} \\mathbf{R},\n",
    "$$\n",
    "where $\\mathbf{Q} \\in \\mathbb{R}^{n \\times p}$ has orthonormal columns and $\\mathbf{R} \\in \\mathbb{R}^{p \\times p}$ is an upper triangular matrix.\n",
    "\n",
    "* **Gram-Schmidt algorithm** orthonormalizes a set of non-zero, *linearly independent* vectors $\\mathbf{x}_1,\\ldots,\\mathbf{x}_p$. \n",
    "\n",
    "    0. Initialize $\\mathbf{q}_1 = \\mathbf{x}_1 / \\|\\mathbf{x}_1\\|_2$\n",
    "    0. For $k=2, \\ldots, p$, \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{v}_k &=& \\mathbf{x}_k - \\mathbf{P}_{\\mathcal{C} \\{\\mathbf{q}_1,\\ldots,\\mathbf{q}_{k-1}\\}} \\mathbf{x}_k = \\mathbf{x}_k -  \\sum_{j=1}^{k-1} \\langle \\mathbf{x}_k, \\mathbf{q}_j \\rangle \\cdot \\mathbf{q}_j \\\\\n",
    "\t\\mathbf{q}_k &=& \\mathbf{v}_k / \\|\\mathbf{v}_k\\|_2\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "* Collectively we have $\\mathbf{X} = \\mathbf{Q} \\mathbf{R}$. \n",
    "    - $\\mathbf{Q} \\in \\mathbb{R}^{n \\times p}$ has orthonormal columns $\\mathbf{q}_k$ thus $\\mathbf{Q}^T \\mathbf{Q} = \\mathbf{I}_p$  \n",
    "    - Where is $\\mathbf{R}$? $\\mathbf{R} = \\mathbf{Q}^T \\mathbf{X}$ has entries $r_{jk} = \\langle \\mathbf{q}_j, \\mathbf{x}_k \\rangle$, which are computed during the Gram-Schmidt process. Note $r_{jk} = 0$ for $j > k$, so $\\mathbf{R}$ is upper triangular.\n",
    "    \n",
    "* In GS algorithm, $\\mathbf{X}$ is over-written by $\\mathbf{Q}$ and $\\mathbf{R}$ is stored in a separate array.\n",
    "\n",
    "* The regular Gram-Schmidt is *unstable* (we loose orthogonality due to roundoff errors) when columns of $\\mathbf{X}$ are collinear.\n",
    "\n",
    "* **Modified Gram-Schmidt** (MGS): after each normalization step of $\\mathbf{v}_k$, we replace columns $\\mathbf{x}_j$, $j>k$, by its residual.\n",
    "\n",
    "* Why MGS is better than GS? Read <http://cavern.uark.edu/~arnold/4353/CGSMGS.pdf>\n",
    "\n",
    "* Computational cost of GS and MGS is $\\sum_{k=1}^p 4n(k-1) \\approx 2np^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR by Householder transform\n",
    "\n",
    "<img src=\"Householder.jpg\" width=\"150\" align=\"center\"/>\n",
    "\n",
    "* This is the algorithm implemented in LAPACK. In other words, **this is the algorithm for solving linear regression in R**.\n",
    "\n",
    "* Assume $\\mathbf{X} = (\\mathbf{x}_1, \\ldots, \\mathbf{x}_p) \\in \\mathbb{R}^{n \\times p}$ has full column rank. \n",
    "\n",
    "* Idea:\n",
    "$$\n",
    "    \\mathbf{H}_{p} \\cdots \\mathbf{H}_2 \\mathbf{H}_1 \\mathbf{X} = \\begin{pmatrix} \\mathbf{R}_1 \\\\ \\mathbf{0} \\end{pmatrix},\n",
    "$$\n",
    "where $\\mathbf{H}_j \\in \\mathbf{R}^{n \\times n}$ are a sequence of Householder transformation matrices.\n",
    "\n",
    "    It yields the **full QR** where $\\mathbf{Q} = \\mathbf{H}_1 \\cdots \\mathbf{H}_p \\in \\mathbb{R}^{n \\times n}$. Recall that GS/MGS only produces the **thin QR** decomposition.\n",
    "\n",
    "* For arbitrary $\\mathbf{v}, \\mathbf{w} \\in \\mathbb{R}^{n}$ with $\\|\\mathbf{v}\\|_2 = \\|\\mathbf{w}\\|_2$, we can construct a **Householder matrix** (or **Householder reflector**)\n",
    "$$\n",
    "    \\mathbf{H} = \\mathbf{I}_n - 2 \\mathbf{u} \\mathbf{u}^T, \\quad \\mathbf{u} = - \\frac{1}{\\|\\mathbf{v} - \\mathbf{w}\\|_2} (\\mathbf{v} - \\mathbf{w}),\n",
    "$$\n",
    "that transforms $\\mathbf{v}$ to $\\mathbf{w}$:\n",
    "$$\n",
    "\t\\mathbf{H} \\mathbf{v} = \\mathbf{w}.\n",
    "$$\n",
    "$\\mathbf{H}$ is symmetric and orthogonal. Calculation of Householder vector $\\mathbf{u}$ costs $4n$ flops for general $\\mathbf{u}$ and $\\mathbf{w}$.\n",
    "\n",
    "<img src=\"./householder_reflector.png\" width=\"450\" align=\"center\"/>\n",
    "\n",
    "* Now choose $\\mathbf{H}_1$ to zero the first column of $\\mathbf{X}$ below diagonal\n",
    "$$\n",
    "\t\\mathbf{H}_1 \\mathbf{x}_1 = \\begin{pmatrix} \\|\\mathbf{x}_{1}\\|_2 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}.\n",
    "$$\n",
    "Take $\\mathbf{H}_2$ to zero the second column below diagonal; ...  \n",
    "\n",
    "<img src=\"./householder_transform_diagram.png\" width=\"250\" align=\"center\"/>  \n",
    "\n",
    "* In general, choose the $j$-th Householder transform $\\mathbf{H}_j = \\mathbf{I}_n - 2 \\mathbf{u}_j \\mathbf{u}_j^T$, where \n",
    "$$\n",
    "     \\mathbf{u}_j = \\begin{pmatrix} \\mathbf{0}_{j-1} \\\\ {\\tilde u}_j \\end{pmatrix}, \\quad {\\tilde u}_j \\in \\mathbb{R}^{n-j+1},\n",
    "$$\n",
    "to zero the $j$-th column below diagonal. $\\mathbf{H}_j$ takes the form\n",
    "$$\n",
    "\t\\mathbf{H}_j = \\begin{pmatrix}\n",
    "\t\\mathbf{I}_{j-1} & \\\\\n",
    "\t& \\mathbf{I}_{n-j+1} - 2 {\\tilde u}_j {\\tilde u}_j^T\n",
    "\t\\end{pmatrix} = \\begin{pmatrix}\n",
    "\t\\mathbf{I}_{j-1} & \\\\\n",
    "\t& {\\tilde H}_{j}\n",
    "\t\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "* Applying a Householder transform $\\mathbf{H} = \\mathbf{I} - 2 \\mathbf{u} \\mathbf{u}^T$ to a matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$\n",
    "$$\n",
    "\t\\mathbf{H} \\mathbf{X} = \\mathbf{X} - 2 \\mathbf{u} (\\mathbf{u}^T \\mathbf{X})\n",
    "$$\n",
    "costs $4np$ flops. **Householder updates never entail explicit formation of the Householder matrices.**\n",
    "\n",
    "* Note applying ${\\tilde H}_j$ to $\\mathbf{X}$ only needs $4(n-j+1)(p-j+1)$ flops.\n",
    "\n",
    "* QR by Householder: $\\mathbf{H}_{p} \\cdots \\mathbf{H}_1 \\mathbf{X} = \\begin{pmatrix} \\mathbf{R}_1 \\\\ \\mathbf{0} \\end{pmatrix}$.\n",
    "\n",
    "* The process is done in place. Upper triangular part of $\\mathbf{X}$ is overwritten by $\\mathbf{R}_1$ and the essential Householder vectors ($\\tilde u_{j1}$ is normalized to 1) are stored in $\\mathbf{X}[j:n,j]$.\n",
    "\n",
    "* At $j$-th stage\n",
    "     0. computing the Householder vector ${\\tilde u}_j$ costs $3(n-j+1)$ flops\n",
    "     0. applying the Householder transform ${\\tilde H}_j$ to the $\\mathbf{X}[j:n, j:p]$ block costs $4(n-j+1)(p-j+1)$ flops  \n",
    "     \n",
    "* In total we need \n",
    "$$\n",
    "\\sum_{j=1}^p [3(n-j+1) + 4(n-j+1)(p-j+1)] \\approx 2np^2 - \\frac 23 p^3 \\quad \\text{flops}.\n",
    "$$\n",
    "\n",
    "* Where is $\\mathbf{Q}$? $\\mathbf{Q} = \\mathbf{H}_1 \\cdots \\mathbf{H}_p$. In some applications, it's necessary to form the orthogonal matrix $\\mathbf{Q}$. \n",
    "\n",
    "    Accumulating $\\mathbf{Q}$ costs another $2np^2 - \\frac 23 p^3$ flops.\n",
    "\n",
    "* When computing $\\mathbf{Q}^T \\mathbf{v}$ or $\\mathbf{Q} \\mathbf{v}$ as in some applications (e.g., solve linear equation using QR), no need to form $\\mathbf{Q}$. Simply apply Householder transforms successively to the vector $\\mathbf{v}$.\n",
    "\n",
    "* Computational cost of Householder QR for linear regression: $2n p^2 - \\frac 23 p^3$ (regression coefficients and $\\hat \\sigma^2$) or more (fitted values, s.e., ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Householder QR with column pivoting\n",
    "\n",
    "Consider rank deficient $\\mathbf{X}$.\n",
    "\n",
    "* At the $j$-th stage, swap the column in $\\mathbf{X}[j:n,j:p]$ with maximum $\\ell_2$ norm to be the pivot column. If the maximum $\\ell_2$ norm is 0, it stops, ending with\n",
    "$$\n",
    "\\mathbf{X} \\mathbf{P} = \\mathbf{Q} \\begin{pmatrix} \\mathbf{R}_{11} & \\mathbf{R}_{12} \\\\ \\mathbf{0}_{(n-r) \\times r} & \\mathbf{0}_{(n-r) \\times (p-r)} \\end{pmatrix},\n",
    "$$\n",
    "where $\\mathbf{P} \\in \\mathbb{R}^{p \\times p}$ is a permutation matrix and $r$ is the rank of $\\mathbf{X}$. QR with column pivoting is rank revealing.\n",
    "\n",
    "* The overhead of re-computing the column norms can be reduced by the property\n",
    "$$\n",
    "\t\\mathbf{Q} \\mathbf{z} = \\begin{pmatrix} \\alpha \\\\ \\omega \\end{pmatrix} \\Rightarrow \\|\\omega\\|_2^2 = \\|\\mathbf{z}\\|_2^2 - \\alpha^2\n",
    "$$\n",
    "for any orthogonal matrix $\\mathbf{Q}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LAPACK and Julia implementation\n",
    "\n",
    "* Julia functions: [qr](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.qr), [qr!](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.qr!), or call LAPACK wrapper functions [geqrf!](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.LAPACK.geqrf!) and [geqp3!](https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.LAPACK.geqp3!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5Ã—3 Matrix{Float64}:\n",
       "  1.52556    0.67424     0.438785\n",
       " -1.69501   -1.48526    -0.535651\n",
       " -0.245347   0.196908   -0.623759\n",
       " -1.41158   -0.0191747  -0.580781\n",
       "  0.270371   0.68845     1.47836"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Random, LinearAlgebra\n",
    "\n",
    "Random.seed!(257) # seed\n",
    "\n",
    "y = randn(5)    # response vector\n",
    "X = randn(5, 3) # predictor matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "  0.7737725680473986\n",
       " -2.1947916892054415\n",
       "  0.45633734842685486"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X \\ y # least squares solution by QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "  0.773772568047398\n",
       " -2.194791689205441\n",
       "  0.45633734842685486"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# same as\n",
    "qr(X) \\ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "  0.7737725680473986\n",
       " -2.194791689205442\n",
       "  0.45633734842685497"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cholesky(X'X) \\ (X'y) # least squares solution by Cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QRPivoted{Float64, Matrix{Float64}, Vector{Float64}, Vector{Int64}}\n",
       "Q factor: 5Ã—5 LinearAlgebra.QRPackedQ{Float64, Matrix{Float64}, Vector{Float64}}\n",
       "R factor:\n",
       "3Ã—3 Matrix{Float64}:\n",
       " -2.70671  -1.08984  -1.37105\n",
       "  0.0      -1.48446  -0.339038\n",
       "  0.0       0.0      -1.08581\n",
       "permutation:\n",
       "3-element Vector{Int64}:\n",
       " 1\n",
       " 3\n",
       " 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# QR factorization with column pivoting\n",
    "xqr = qr(X, Val(true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       "  0.7737725680473986\n",
       " -2.1947916892054415\n",
       "  0.45633734842685486"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xqr \\ y # least squares solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.368045037175248e-16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# thin Q matrix multiplication (a sequence of Householder transforms)\n",
    "norm(xqr.Q * xqr.R - X[:, xqr.p]) # recovers X (with columns permuted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR by Givens rotation\n",
    "\n",
    "* Householder transform $\\mathbf{H}_j$ introduces batch of zeros into a vector.\n",
    "\n",
    "* **Givens transform** (aka **Givens rotation**, **Jacobi rotation**, **plane rotation**) selectively zeros one element of a vector.\n",
    "\n",
    "* Overall QR by Givens rotation is less efficient than the Householder method, but is better suited for matrices with structured patterns of nonzero elements.\n",
    "\n",
    "* **Givens/Jacobi rotations**: \n",
    "$$\n",
    "\t\\mathbf{G}(i,k,\\theta) = \\begin{pmatrix} \n",
    "\t1 & & 0 & & 0 & & 0 \\\\\n",
    "\t\\vdots & \\ddots & \\vdots & & \\vdots & & \\vdots \\\\\n",
    "\t0 & & c & & s & & 0 \\\\ \n",
    "\t\\vdots & & \\vdots & \\ddots & \\vdots & & \\vdots \\\\\n",
    "\t0 & & - s & & c & & 0 \\\\\n",
    "\t\\vdots & & \\vdots & & \\vdots & \\ddots & \\vdots \\\\\n",
    "\t0 & & 0 & & 0 & & 1 \\end{pmatrix},\n",
    "$$\n",
    "where $c = \\cos(\\theta)$ and $s = \\sin(\\theta)$. $\\mathbf{G}(i,k,\\theta)$ is orthogonal.\n",
    "\n",
    "* Pre-multiplication by $\\mathbf{G}(i,k,\\theta)^T$ rotates counterclockwise $\\theta$ radians in the $(i,k)$ coordinate plane. If $\\mathbf{x} \\in \\mathbb{R}^n$ and $\\mathbf{y} = \\mathbf{G}(i,k,\\theta)^T \\mathbf{x}$, then\n",
    "$$\n",
    "\ty_j = \\begin{cases}\n",
    "\tcx_i - s x_k & j = i \\\\\n",
    "\tsx_i + cx_k & j = k \\\\\n",
    "\tx_j & j \\ne i, k\n",
    "\t\\end{cases}.\n",
    "$$\n",
    "Apparently if we choose $\\tan(\\theta) = -x_k / x_i$, or equivalently,\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\tc = \\frac{x_i}{\\sqrt{x_i^2 + x_k^2}}, \\quad s = \\frac{-x_k}{\\sqrt{x_i^2 + x_k^2}},\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "then $y_k=0$.\n",
    "\n",
    "* Pre-applying Givens transform $\\mathbf{G}(i,k,\\theta)^T \\in \\mathbb{R}^{n \\times n}$ to a matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$ only effects two rows of $\\mathbf{\n",
    "A}$:\n",
    "$$\n",
    "\t\\mathbf{A}([i, k], :) \\gets \\begin{pmatrix} c & s \\\\ -s & c \\end{pmatrix}^T \\mathbf{A}([i, k], :),\n",
    "$$\n",
    "costing $6m$ flops.\n",
    "\n",
    "* Post-applying Givens transform $\\mathbf{G}(i,k,\\theta) \\in \\mathbb{R}^{m \\times m}$ to a matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$ only effects two columns of $\\mathbf{A}$:\n",
    "$$\n",
    "\t\\mathbf{A}(:, [i,k]) \\gets \\mathbf{A}(:, [i,k]) \\begin{pmatrix} c & s \\\\ -s & c \\end{pmatrix},\n",
    "$$\n",
    "costing $6n$ flops.\n",
    "\n",
    "* QR by Givens: $\\mathbf{G}_t^T \\cdots \\mathbf{G}_1^T \\mathbf{X} =  \\begin{pmatrix} \\mathbf{R}_1 \\\\ \\mathbf{0} \\end{pmatrix}$.\n",
    "\n",
    "<img src=\"QR_by_Givens.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "* Zeros in $\\mathbf{X}$ can also be introduced row-by-row.\n",
    "\n",
    "* If $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$, the total cost is $3np^2 - p^3$ flops and $O(np)$ square roots.\n",
    "\n",
    "* Note each Givens transform can be summarized by a single number, which is stored in the zeroed entry of $\\mathbf{X}$.\n",
    "\n",
    "* *Fast Givens transform* avoids taking square roots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression (ad done in R)\n",
    "\n",
    "* QR decomposition $\\mathbf{X} = \\mathbf{Q} \\mathbf{R}$: $2np^2 - \\frac 23 p^3$ flops.\n",
    "\n",
    "* Solve $\\mathbf{R}^T \\mathbf{R} \\beta = \\mathbf{R}^T \\mathbf{Q}^T \\mathbf{y}$ for $\\beta$.\n",
    "\n",
    "* If need standard errors, compute inverse of $\\mathbf{R}^T \\mathbf{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haar measure on orthogonal group\n",
    "\n",
    "* How to generate an orthogonal matrix \"uniformly\"?\n",
    "\n",
    "* The **Haar measure** is a probability measure $\\mathbb{P}$ on a group G which is translation invariant: for any measurable set $A$ in $G$ and any element $M$ in $G$,\n",
    "$$\n",
    "\\mathbb{P}(A) = \\mathbb{P}(MA).\n",
    "$$\n",
    "\n",
    "* Consider the orthogonal group $\\mathcal{O}_n = \\{\\mathbf{M} \\in \\mathbb{R}^{n \\times n}: \\mathbf{M}^T \\mathbf{M} = \\mathbf{I}_n\\}$. How to generate a random orthogonal matrix from the Haar measure on $\\mathcal{O}_n$? \n",
    "\n",
    "* Here is a simple algorithm: generate a random matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ with iid standard normal entries, and run QR decomposition $\\mathbf{A} = \\mathbf{Q} \\mathbf{R}$. Then $\\mathbf{Q}$ is distributed according to the Haar measure.\n",
    "\n",
    "    See the article [_PATTERNS IN EIGENVALUES: THE 70TH JOSIAH WILLARD GIBBS LECTURE_](https://doi.org/10.1090%2Fs0273-0979-03-00975-3) (2003) by Persi Diaconis for the proof and statistical applications of this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "* Section 7.8 of [Numerical Analysis for Statisticians](http://ucla.worldcat.org/title/numerical-analysis-for-statisticians/oclc/793808354&referer=brief_results) of Kenneth Lange (2010).\n",
    "\n",
    "* Section II.5.3 of [Computational Statistics](http://ucla.worldcat.org/title/computational-statistics/oclc/437345409&referer=brief_results) by James Gentle (2010).\n",
    "\n",
    "* Chapter 5 of [Matrix Computation](https://search.library.ucla.edu/permalink/01UCS_LAL/17p22dp/alma9971220883606533) by Gene Golub and Charles Van Loan (2013)."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "jupytext": {
   "formats": "ipynb,qmd"
  },
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "65px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
