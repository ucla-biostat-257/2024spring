---
layout: post_teaching
title: Lecture 2
category: biostat257spring2024
---

## Announcement

* Homework 1 posted. Due Friday 4/12.

## Today

* Getting started with JupyterLab, VS Code, HW 1.

* Computer languages.

## Readings

* Julia introduction notes.

* Julia plotting notes.

* Jupyter notebook/lab notes.

## Q&A

* Q: In `BenchmarkTools.jl`, what's the relation between `sample` and `evaluation`?

A: From the [documentation](https://juliaci.github.io/BenchmarkTools.jl/stable/manual/),

> The reasoning behind our definition of "sample" may not be obvious to all readers. If the time to execute a benchmark is smaller than the resolution of your timing method, then a single evaluation of the benchmark will generally not produce a valid sample. In that case, one must approximate a valid sample by recording the total time t it takes to record n evaluations, and estimating the sample's time per evaluation as t/n. For example, if a sample takes 1 second for 1 million evaluations, the approximate time per evaluation for that sample is 1 microsecond. It's not obvious what the right number of evaluations per sample should be for any given benchmark, so BenchmarkTools provides a mechanism (the tune! method) to automatically figure it out for you.

